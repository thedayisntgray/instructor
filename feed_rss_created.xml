<?xml version="1.0" encoding="UTF-8" ?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"> <channel><title>Instructor</title><description>Enhancing OpenAI function calling with Pydantic</description><link>https://jxnl.github.io/instructor/</link><atom:link href="https://jxnl.github.io/instructor/feed_rss_created.xml" rel="self" type="application/rss+xml" /><managingEditor>Jason Liu</managingEditor><docs>https://github.com/jxnl/instructor/</docs><language>en</language> <pubDate>Thu, 18 Jan 2024 01:49:36 -0000</pubDate> <lastBuildDate>Thu, 18 Jan 2024 01:49:36 -0000</lastBuildDate> <ttl>1440</ttl> <generator>MkDocs RSS plugin - v1.12.0</generator> <item> <title>Structured Outputs with Anyscale</title> <author>anmol</author> <author>jxnl</author> <category>open source</category> <category>patching</category> <description>&lt;h1&gt;Structured Outputs with Anyscale&lt;/h1&gt;&lt;p&gt;Open-source LLMS are gaining popularity, and the release of Anyscale&#39;s Mistral model has made it possible to obtain structured outputs using JSON schema at any scale. Instead of relying on a model&#39;s default output mode, you can utilize JSON schema to obtain structured outputs. This approach is a time-saving alternative to extensive prompt engineering.&lt;/p&gt;&lt;p&gt;By the end of this blog post, you will learn how to effectively utilize the instructor at any scale. But before we proceed, let&#39;s first explore the concept of patching.&lt;/p&gt;</description><link>https://jxnl.github.io/instructor/blog/2023/12/15/patching/</link> <pubDate>Fri, 15 Dec 2023 00:00:00 +0000</pubDate><source url="https://jxnl.github.io/instructor/feed_rss_created.xml">Instructor</source><guid isPermaLink="true">https://jxnl.github.io/instructor/blog/2023/12/15/patching/</guid> <enclosure url="https://jxnl.github.io/instructor/assets/images/social/blog/posts/anyscale.png" type="image/png" length="45051" /> </item> <item> <title>Introduction to Caching in Python</title> <author>jxnl</author> <category>caching</category> <category>diskcache</category> <category>functools</category> <category>python</category> <category>redis</category> <description>&lt;h1&gt;Introduction to Caching in Python&lt;/h1&gt;&lt;blockquote&gt;&lt;p&gt;Instructor makes working with language models easy, but they are still computationally expensive.&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;Today, we&#39;re diving into optimizing instructor code while maintaining the excellent DX offered by &lt;a href=&#34;https://docs.pydantic.dev/latest/&#34;&gt;Pydantic&lt;/a&gt; models. We&#39;ll tackle the challenges of caching Pydantic models, typically incompatible with &lt;code&gt;pickle&lt;/code&gt;, and explore solutions that use &lt;code&gt;decorators&lt;/code&gt; like &lt;code&gt;functools.cache&lt;/code&gt;. Then, we&#39;ll craft custom decorators with &lt;code&gt;diskcache&lt;/code&gt; and &lt;code&gt;redis&lt;/code&gt; to support persistent caching and distributed systems.&lt;/p&gt;</description><link>https://jxnl.github.io/instructor/blog/2023/11/26/python-caching/</link> <pubDate>Sun, 26 Nov 2023 00:00:00 +0000</pubDate><source url="https://jxnl.github.io/instructor/feed_rss_created.xml">Instructor</source><guid isPermaLink="true">https://jxnl.github.io/instructor/blog/2023/11/26/python-caching/</guid> <enclosure url="https://jxnl.github.io/instructor/assets/images/social/blog/posts/caching.png" type="image/png" length="38856" /> </item> <item> <title>Generators and LLM Streaming</title> <author>jxnl</author> <author>anmol</author> <category>generators</category> <category>python</category> <category>streaming</category> <description>&lt;h1&gt;Generators and LLM Streaming&lt;/h1&gt;&lt;p&gt;Latency is crucial, especially in eCommerce and newer chat applications like ChatGPT. Streaming is the solution that enables us to enhance the user experience without the need for faster response times.&lt;/p&gt;&lt;p&gt;And what makes streaming possible? Generators!&lt;/p&gt;</description><link>https://jxnl.github.io/instructor/blog/2023/11/26/python-generators-and-llm-streaming/</link> <pubDate>Sun, 26 Nov 2023 00:00:00 +0000</pubDate><source url="https://jxnl.github.io/instructor/feed_rss_created.xml">Instructor</source><guid isPermaLink="true">https://jxnl.github.io/instructor/blog/2023/11/26/python-generators-and-llm-streaming/</guid> <enclosure url="https://jxnl.github.io/instructor/assets/images/social/blog/posts/generator.png" type="image/png" length="41822" /> </item> <item> <title>Verifying LLM Citations with Pydantic</title> <author>jxnl</author> <category>citations</category> <category>finetuneing</category> <category>hallucination</category> <category>pydantic</category> <category>validation</category> <description>&lt;h1&gt;Verifying LLM Citations with Pydantic&lt;/h1&gt;&lt;p&gt;Ensuring the accuracy of information is crucial. This blog post explores how Pydantic&#39;s powerful and flexible validators can enhance data accuracy through citation verification.&lt;/p&gt;&lt;p&gt;We&#39;ll start with using a simple substring check to verify citations. Then we&#39;ll use &lt;code&gt;instructor&lt;/code&gt; itself to power an LLM to verify citations and align answers with the given citations. Finally, we&#39;ll explore how we can use these techniques to generate a dataset of accurate responses.&lt;/p&gt;</description><link>https://jxnl.github.io/instructor/blog/2023/11/18/validate-citations/</link> <pubDate>Sat, 18 Nov 2023 00:00:00 +0000</pubDate><source url="https://jxnl.github.io/instructor/feed_rss_created.xml">Instructor</source><guid isPermaLink="true">https://jxnl.github.io/instructor/blog/2023/11/18/validate-citations/</guid> <enclosure url="https://jxnl.github.io/instructor/assets/images/social/blog/posts/citations.png" type="image/png" length="47453" /> </item> <item> <title>Introduction to Batch Processing using asyncio and Instructor</title> <author>jxnl</author> <category>async</category> <category>async/await</category> <category>asyncio</category> <category>batch</category> <category>python</category> <description>&lt;h1&gt;Introduction to Batch Processing using &lt;code&gt;asyncio&lt;/code&gt; and &lt;code&gt;Instructor&lt;/code&gt;&lt;/h1&gt;&lt;p&gt;Today, I will introduce you to various approaches for using asyncio in Python. We will apply this to batch process data using &lt;code&gt;instructor&lt;/code&gt; and learn how to use &lt;code&gt;asyncio.gather&lt;/code&gt; and &lt;code&gt;asyncio.as_completed&lt;/code&gt; for concurrent data processing. Additionally, we will explore how to limit the number of concurrent requests to a server using &lt;code&gt;asyncio.Semaphore&lt;/code&gt;.&lt;/p&gt;</description><link>https://jxnl.github.io/instructor/blog/2023/11/13/learn-async/</link> <pubDate>Mon, 13 Nov 2023 00:00:00 +0000</pubDate><source url="https://jxnl.github.io/instructor/feed_rss_created.xml">Instructor</source><guid isPermaLink="true">https://jxnl.github.io/instructor/blog/2023/11/13/learn-async/</guid> <enclosure url="https://jxnl.github.io/instructor/assets/images/social/blog/posts/learn-async.png" type="image/png" length="53373" /> </item> <item> <title>Smarter Summaries w/ Finetuning GPT-3.5 and Chain of Density</title> <author>ivanleomk</author> <author>jxnl</author> <category>chain of density</category> <category>distillation</category> <category>finetuneing</category> <category>gpt-3.5-turbo</category> <category>pydantic</category> <category>validation</category> <description>&lt;h1&gt;Smarter Summaries w/ Finetuning GPT-3.5 and Chain of Density&lt;/h1&gt;&lt;blockquote&gt;&lt;p&gt;Discover how to distil an iterative method like Chain Of Density into a single finetuned model using Instructor&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;In this article, we&#39;ll guide you through implementing the original Chain of Density method using Instructor, then show how to distile a GPT 3.5 model to match GPT-4&#39;s iterative summarization capabilities. Using these methods were able to decrease latency by 20x, reduce costs by 50x and maintain entity density.&lt;/p&gt;&lt;p&gt;By the end you&#39;ll end up with a GPT 3.5 model, (fine-tuned using Instructor&#39;s great tooling), capable of producing summaries that rival the effectiveness of Chain of Density &lt;a href=&#34;https://arxiv.org/abs/2309.04269&#34;&gt;[Adams et al. (2023)]&lt;/a&gt;. As always, all code is readily available in our &lt;code&gt;examples/chain-of-density&lt;/code&gt; folder in our repo for your reference.&lt;/p&gt;</description><link>https://jxnl.github.io/instructor/blog/2023/11/05/chain-of-density/</link> <pubDate>Sun, 05 Nov 2023 00:00:00 +0000</pubDate><source url="https://jxnl.github.io/instructor/feed_rss_created.xml">Instructor</source><guid isPermaLink="true">https://jxnl.github.io/instructor/blog/2023/11/05/chain-of-density/</guid> <enclosure url="https://jxnl.github.io/instructor/assets/images/social/blog/posts/chain-of-density.png" type="image/png" length="50256" /> </item> <item> <title>AI Engineer Keynote: Pydantic is all you need</title> <author>jxnl</author> <category>prompt engineering</category> <category>python</category> <category>talks</category> <category>video</category> <description>&lt;h1&gt;AI Engineer Keynote: Pydantic is all you need&lt;/h1&gt;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=yj-wSRJwrrc&#34;&gt;&lt;img alt=&#34;Pydantic is all you need&#34; src=&#34;https://img.youtube.com/vi/yj-wSRJwrrc/0.jpg&#34;&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=yj-wSRJwrrc&#34;&gt;Click here to watch the full talk&lt;/a&gt;&lt;/p&gt;</description><link>https://jxnl.github.io/instructor/blog/2023/11/02/ai-engineer-keynote-pydantic-is-all-you-need/</link> <pubDate>Thu, 02 Nov 2023 00:00:00 +0000</pubDate><source url="https://jxnl.github.io/instructor/feed_rss_created.xml">Instructor</source><guid isPermaLink="true">https://jxnl.github.io/instructor/blog/2023/11/02/ai-engineer-keynote-pydantic-is-all-you-need/</guid> <enclosure url="https://jxnl.github.io/instructor/assets/images/social/blog/posts/aisummit-2023.png" type="image/png" length="49811" /> </item> <item> <title>Good LLM Validation is Just Good Validation</title> <author>jxnl</author> <author>ivanleomk</author> <category>chain of thought</category> <category>citations</category> <category>constitutional ai</category> <category>guardrails</category> <category>pydantic</category> <category>validation</category> <description>&lt;h1&gt;Good LLM Validation is Just Good Validation&lt;/h1&gt;&lt;blockquote&gt;&lt;p&gt;What if your validation logic could learn and adapt like a human, but operate at the speed of software? This is the future of validation and it&#39;s already here.&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;Validation is the backbone of reliable software. But traditional methods are static, rule-based, and can&#39;t adapt to new challenges. This post looks at how to bring dynamic, machine learning-driven validation into your software stack using Python libraries like &lt;code&gt;Pydantic&lt;/code&gt; and &lt;code&gt;Instructor&lt;/code&gt;. We validate these outputs using a validation function which conforms to the structure seen below.&lt;/p&gt;&lt;p&gt;&lt;code&gt;pythondef validation_function(value): if condition(value): raise ValueError(&#34;Value is not valid&#34;) return mutation(value)&lt;/code&gt;&lt;/p&gt;</description><link>https://jxnl.github.io/instructor/blog/2023/10/23/good-llm-validation-is-just-good-validation/</link> <pubDate>Mon, 23 Oct 2023 00:00:00 +0000</pubDate><source url="https://jxnl.github.io/instructor/feed_rss_created.xml">Instructor</source><guid isPermaLink="true">https://jxnl.github.io/instructor/blog/2023/10/23/good-llm-validation-is-just-good-validation/</guid> <enclosure url="https://jxnl.github.io/instructor/assets/images/social/blog/posts/validation-part1.png" type="image/png" length="46286" /> </item> <item> <title>Enhancing Python Functions with Instructor: A Guide to Fine-Tuning and Distillation</title> <author>jxnl</author> <category>distillation</category> <category>finetuning</category> <category>function calling</category> <category>python</category> <description>&lt;h1&gt;Enhancing Python Functions with Instructor: A Guide to Fine-Tuning and Distillation&lt;/h1&gt;&lt;h2&gt;Introduction&lt;/h2&gt;&lt;p&gt;Get ready to dive deep into the world of fine-tuning task specific language models with Python functions. We&#39;ll explore how the &lt;code&gt;instructor.instructions&lt;/code&gt; streamlines this process, making the task you want to distil more efficient and powerful while preserving its original functionality and backwards compatibility.&lt;/p&gt;&lt;p&gt;If you want to see the full example checkout &lt;a href=&#34;https://github.com/jxnl/instructor/tree/main/examples/distilations&#34;&gt;examples/distillation&lt;/a&gt;&lt;/p&gt;</description><link>https://jxnl.github.io/instructor/blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/</link> <pubDate>Tue, 17 Oct 2023 00:00:00 +0000</pubDate><source url="https://jxnl.github.io/instructor/feed_rss_created.xml">Instructor</source><guid isPermaLink="true">https://jxnl.github.io/instructor/blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/</guid> <enclosure url="https://jxnl.github.io/instructor/assets/images/social/blog/posts/distilation-part1.png" type="image/png" length="53207" /> </item> <item> <title>RAG is more than just embedding search</title> <author>jxnl</author> <category>Embeddings</category> <category>Personal Assistant</category> <category>Query Understanding</category> <category>RAG</category> <category>Search Systems</category> <description>&lt;h1&gt;RAG is more than just embedding search&lt;/h1&gt;&lt;p&gt;With the advent of large language models (LLM), retrieval augmented generation (RAG) has become a hot topic. However throughout the past year of &lt;a href=&#34;https://jxnl.co&#34;&gt;helping startups&lt;/a&gt; integrate LLMs into their stack I&#39;ve noticed that the pattern of taking user queries, embedding them, and directly searching a vector store is effectively demoware.&lt;/p&gt;&lt;p&gt;!!! note &#34;What is RAG?&#34;&lt;/p&gt;&lt;pre&gt;&lt;code&gt;Retrieval augmented generation (RAG) is a technique that uses an LLM to generate responses, but uses a search backend to augment the generation. In the past year using text embeddings with a vector databases has been the most popular approach I&#39;ve seen being socialized.&lt;/code&gt;&lt;/pre&gt;&lt;figure markdown&gt; ![RAG](img/dumb_rag.png) &lt;figcaption&gt;Simple RAG that embedded the user query and makes a search.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;So let&#39;s kick things off by examining what I like to call the &#39;Dumb&#39; RAG Modelâ€”a basic setup that&#39;s more common than you&#39;d think.&lt;/p&gt;</description><link>https://jxnl.github.io/instructor/blog/2023/09/17/rag-is-more-than-just-embedding-search/</link> <pubDate>Sun, 17 Sep 2023 00:00:00 +0000</pubDate><source url="https://jxnl.github.io/instructor/feed_rss_created.xml">Instructor</source><guid isPermaLink="true">https://jxnl.github.io/instructor/blog/2023/09/17/rag-is-more-than-just-embedding-search/</guid> <enclosure url="https://jxnl.github.io/instructor/assets/images/social/blog/posts/rag-and-beyond.png" type="image/png" length="51289" /> </item> <item> <title>Bridging Language Models with Python using Instructor, Pydantic, and OpenAI&#39;s Function Calls</title> <author>jxnl</author> <category>Introduction</category> <description>&lt;h1&gt;Bridging Language Models with Python using Instructor, Pydantic, and OpenAI&#39;s Function Calls&lt;/h1&gt;&lt;p&gt;Language models have seen significant growth. Using them effectively often requires complex frameworks. This post discusses how Instructor simplifies this process using Pydantic.&lt;/p&gt;</description><link>https://jxnl.github.io/instructor/blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/</link> <pubDate>Mon, 11 Sep 2023 00:00:00 +0000</pubDate><source url="https://jxnl.github.io/instructor/feed_rss_created.xml">Instructor</source><guid isPermaLink="true">https://jxnl.github.io/instructor/blog/2023/09/11/bridging-language-models-with-python-using-instructor-pydantic-and-openais-function-calls/</guid> <enclosure url="https://jxnl.github.io/instructor/assets/images/social/blog/posts/introduction.png" type="image/png" length="52766" /> </item> </channel></rss>